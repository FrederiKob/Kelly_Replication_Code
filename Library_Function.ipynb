{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEwLLCxnVvVIm9O9eDvtc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrederiKob/Kelly_Replication_Code/blob/main/Library_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "r5TvyPuNXbMb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Function Script includes the following functions:**\n",
        "\n",
        "\n",
        "1.   Generate w_i --> generates random N(0,1) draws used in the RFFs (*generate_w_i*)\n",
        "2.   Generate Signals --> generates RFF's for a specific seed and for a specific number of P's (*generate_Signals*)\n",
        "3.   Generate X and y samples used for fitting and predicting (*generate_X_y*)\n",
        "4.   Run predictions over all t's and over multiple iterations (*run_all*)"
      ],
      "metadata": {
        "id": "BMVsFys8t-zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generate w_i:\n",
        "Takes as input:\n",
        "*   max_iterations: number of iterations to average predictions over\n",
        "*   max_P: maximum nuber of P's used to generate RFF's (only need half since we are using cos and sin)\n",
        "*   will always be run using seeds from range(0, max_iterations) --> replicability guaranteed\n",
        "\n",
        "From Kelly et al. (2022) *The Virtue Complexity Everywhere* page 21:\n",
        "\"[...] given the signal vector $X_{t} = \\mathbb{R}^{d}$ [...] fix a random seed $s$ and sample random weights $w_{i}(s) \\in \\mathbb{R}^{d}$ from $\\mathcal{N} \\sim (0,I_{dxd})$ \"\n",
        "\n",
        "Hence, the w_i gets drawn from multivariate_normal mean = 0; covariance matrix = Identity.\n",
        "Resulting in a dictionary with max_iteration number of keys (name  with every key containing a matrix of shape max_P/2 x number of predictors (G = 15).\n",
        "\n",
        "=> every matrix is generated by the specific seed of range(max_iterations), thus the random draws and predicitons are easily replicated and can be used for all P-T-alpha generations.\n",
        ""
      ],
      "metadata": {
        "id": "Jpxe2kBZMf6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_w_i(max_iterations = None, max_P = None):\n",
        "    dic_wi = dict.fromkeys(range(max_iterations))\n",
        "    \"\"\" Always set seed to 0 --> generate max_iteration verisons of random draws used\n",
        "        --> one set of these random w_i's is used for one iteration\n",
        "    \"\"\"\n",
        "    for ite in range(max_iterations):\n",
        "        np.random.seed(ite)\n",
        "        # round so that sufficient w_i are drawn in case max_P is odd\n",
        "        w_i = np.random.multivariate_normal(mean = np.array([0]*15), cov = np.identity(15), size = int(round(max_P/2)))\n",
        "        dic_wi[ite] = w_i\n",
        "    return dic_wi"
      ],
      "metadata": {
        "id": "g64fzhoAMdYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. generate_signals:\n",
        "*   pred_std: standardized predictor variables used to generate signals (lagged market return is included)\n",
        "*   P: number of predictors to generate $P \\in [2,...,12000]$\n",
        "\n",
        "The signals or RFFs are generated as follows (see Kelly et al. (2022)):\n",
        " $S_{i,t}(s) = \\frac{1}{P^{1/2}}\\left [ sin(\\gamma w_{i}(s)'X_{t}), cos(\\gamma w_{i}(s)' X_{t}) \\right ]', i = 1,\\cdots, P$,\n",
        "\n",
        " where \"[...] the parameter $\\gamma$ controls the Gaussian kernel bandwitdth in the generation of random Fourier features [...] we set $\\gamma = 2$. Our results are generally insensitive to $\\gamma$...\" (page 39-40)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2yQtMvPgMy-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_Signals(pred_std = None, P = None, use_seed = None, w_i_all = None):\n",
        "    # gamma = 2\n",
        "    gamma = 2\n",
        "    # use seed picks out dicitonary entries of w_i generated with different seeds in order to use a specific seed = use_seed\n",
        "    w_i = w_i_all[use_seed][:int(round(P/2))]\n",
        "    inside = gamma * w_i @ pred_std.T\n",
        "    # using trig-function\n",
        "    sig = (P**(-0.5) * np.cos(inside.T), P**(-0.5) * np.sin(inside.T))\n",
        "    signals = np.concatenate([sig[0], sig[1]], axis=1)\n",
        "    # check if even or odd\n",
        "    if P % 2 != 0:\n",
        "        # random item to drop --> due to one signal too many\n",
        "        drop = np.random.randint(0, P+1)\n",
        "        # drop random element --> correct size of signals\n",
        "        signals = np.matrix([np.delete(i, drop, 0) for i in signals])\n",
        "    else:\n",
        "        signals = np.matrix(signals)\n",
        "\n",
        "    # return signals\n",
        "    return signals\n"
      ],
      "metadata": {
        "id": "eg9lrzR1M7NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Generate X_y:\n",
        "\n",
        "\n",
        "\n",
        "*   In line with Kelly et al. page 40: \"Prior to estimation, we volatility standardize the training sample RFFs ${S_{t-1}, ..., S_{t-T}}$ and out-of-sample RFFs $S_{t}$ by their standard deviations in the training sample.\"\n",
        "*   Option for vol_stand is used for Goyal and Welch replication --> irrelevant here\n",
        "*   Pull data from ret_std (target excess returns) and pred_std (volatility standardized predictors) at position idx_start (date position of forecast). In order to assure sufficient observations for different T we go back T observations.\n",
        "*   Returns two tuples containing train and test samples for X and y for ridge regression."
      ],
      "metadata": {
        "id": "frx0joEoM_VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Option_generate_X_y(signals = None, ret_std = None, idx_start = None, T = None, P = None, vol_stand = True):\n",
        "    # pull T+1 observations --> T observations for train sample and idx_start is the target which we want to forecast\n",
        "    X = signals[idx_start-T:idx_start+1]\n",
        "    Y = ret_std[idx_start-T:idx_start+1]\n",
        "\n",
        "    X_train = X[:-1,:]\n",
        "    y_train = Y[:-1]\n",
        "\n",
        "    X_test = X[-1,:].reshape(1,-1)\n",
        "    y_test = Y[-1]\n",
        "\n",
        "    if vol_stand:\n",
        "        std = X_train.std(ddof=0, axis=0)\n",
        "        X_train = X_train/std\n",
        "        X_test = X_test/std\n",
        "        return (X_train,X_test),(y_train,y_test)\n",
        "    else:\n",
        "        return (X_train,X_test),(y_train,y_test)"
      ],
      "metadata": {
        "id": "cnuEX10aNEGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "This section contains functions that are not used in the rudimentary replicaiton code!"
      ],
      "metadata": {
        "id": "nsTNzqhl2qPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. pull_data:\n",
        "\n",
        "*   Pull all results (depending on pull_all) that are relevant for a pre-determined set of parameters. Reads all the files in a local directory and macthes with given parameters to read-in current progress and result.\n",
        "*   Necessary due to long runtimes of parameter-combinations and risk of program/PC crashing\n",
        "*   Dependent on path_read which is the directory in which the files are stored\n",
        "*   Pulls results and coefficient data for $||\\hat{\\beta}^{2}||$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CsIgn9n229vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_data(params = None, max_iterations = None, path_read = None, pull_all = False):\n",
        "    ## all possible combinations (ParaGrid)\n",
        "    combs = list(itertools.product(params[\"T_grid\"], params[\"P_grid\"], params[\"a_grid\"]))\n",
        "    names_a = [\"Results_T{}_P{}_a{}.feather\".format(i[0],i[1],i[2]) for i in combs]\n",
        "    names_b = [\"Coef_T{}_P{}_a{}.feather\".format(i[0],i[1],i[2]) for i in combs]\n",
        "    names = names_a, names_b, combs\n",
        "\n",
        "    ## Dictionary containing results\n",
        "    results = dict.fromkeys(combs)\n",
        "    coef = dict.fromkeys(combs)\n",
        "\n",
        "    # Check which files exist\n",
        "    for n,m,c in zip(*names):\n",
        "        e = os.path.isfile(path_read + n)\n",
        "        f = os.path.isfile(path_read + m)\n",
        "        # if both exist\n",
        "        if e and f:\n",
        "            # if file exists --> load in file\n",
        "            df_n = pd.read_feather(path_read + n)\n",
        "            df_n.index = df_n.iloc[:,0]\n",
        "            df_n = df_n.drop(\"yyyymm\", axis=1)\n",
        "\n",
        "            df_m = pd.read_feather(path_read + m)\n",
        "            df_m.index = df_m.iloc[:,0]\n",
        "            df_m = df_m.drop(\"yyyymm\", axis=1)\n",
        "\n",
        "            ## if pull all\n",
        "            if pull_all:\n",
        "                results[c] = df_n\n",
        "                coef[c] = df_m\n",
        "            else:\n",
        "                ## check if max_iteration has been reached --> if yes, do not write\n",
        "                if df_n.shape[1] <= max_iterations: results[c] = df_n\n",
        "                if df_m.shape[1] <= max_iterations: coef[c] = df_m\n",
        "\n",
        "    return results, coef"
      ],
      "metadata": {
        "id": "9WBwoZ_W2_-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. to_do:\n",
        "*   Determine progress of results and coefs pulled in previous function\n",
        "*   Compares the params used as input and the max_iterations in order to determine which parameter-combinations are not yet finished.\n",
        "*   Output gives the parameter-combinations with the least progress and the number of iterations it has already run.\n",
        "*   Progress is important so that the correct seed can be applied to future iterations.\n",
        "\n",
        "=> Will always begin with the parameter-combinations that still need x number of iterations to catch up to other parameter-combinations. After having caught up, run all parameter-combinations together.\n",
        "\n"
      ],
      "metadata": {
        "id": "I3o1NA5U8a-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_do(results = None, params = None, max_iterations = None):\n",
        "    # Check what still needs doing --> assume coef and results have same progress (dependent on params) --> some might not exist\n",
        "    progress = []\n",
        "    for k,v in results.items():\n",
        "        if v is None:\n",
        "            progress.append([0,k])\n",
        "        elif v.shape[1] < max_iterations:\n",
        "            progress.append([v.shape[1],k])\n",
        "\n",
        "    # which combinations need to be run to catch up\n",
        "    unique = np.unique([i[0] for i in progress])\n",
        "    unique.sort()\n",
        "    # if unique is empty list/sequence --> job is done for given params and max_iterations\n",
        "    if len(unique) == 0:\n",
        "        return 0\n",
        "    elif len(unique) == 1:\n",
        "        iters_needed = (max_iterations - unique)[0]\n",
        "    else:\n",
        "        # determine the least number of iterations to catch-up to progress of second-least batch\n",
        "        # --> necessary in order to uniquely assign seed\n",
        "        iters_needed = unique[1] - unique[0]\n",
        "\n",
        "    # minimum progress (for numerically assigning seed_use)\n",
        "    min_progress = min(unique)\n",
        "    to_do = []\n",
        "    for i in progress:\n",
        "        if i[0] == min_progress: to_do.append(i[1])\n",
        "    # at which seed to start and iterations needed\n",
        "    seeds_use = (min_progress,iters_needed)\n",
        "\n",
        "    return (seeds_use, to_do)"
      ],
      "metadata": {
        "id": "95JlFfkU8beb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. run_all:\n"
      ],
      "metadata": {
        "id": "J7TepIoCNJtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all(ret_std = None, pred_std = None, w_i_all = None, params = None, operations = None, end = None):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    # combinations to run (taken from operations)\n",
        "    combs = operations[1]\n",
        "    # seeds and iterations needed\n",
        "    seed_start, iters_needed = operations[0]\n",
        "    # append results to\n",
        "    dic_res = {i:[] for i in combs}\n",
        "    dic_coef = {i:[] for i in combs}\n",
        "    # at what index to stop\n",
        "    idx_end = np.where(ret_std.index == end)[0][0]\n",
        "    # define counter\n",
        "    counter = seed_start\n",
        "    counter_stop = seed_start + iters_needed\n",
        "\n",
        "    # Start outer loop (over iterations)\n",
        "    while counter < counter_stop:\n",
        "        print(\"The current counter is: {} out of {} iterations\".format(counter,counter_stop))\n",
        "\n",
        "        # Start second loop over the combinations\n",
        "        for c in combs:\n",
        "          # parameters\n",
        "          T,P,alpha = c\n",
        "          # generate P-specific signals (used for all P)\n",
        "          signals = generate_Signals(pred_std=pred_std, P=P, use_seed=counter, w_i_all=w_i_all)\n",
        "          # lst to append predictions to\n",
        "          lst_res,lst_coef = [],[]\n",
        "\n",
        "          # Start third loop over all observations in T\n",
        "          for t in range(T,idx_end+1): # CHECK IF THIS CORRECT\n",
        "              # generate XY\n",
        "              x,y = generate_X_y(signals=signals,ret_std=ret_std, idx_start=t, T=T, P=P)\n",
        "              # fit and predict\n",
        "              clf = Ridge(alpha=alpha, fit_intercept=False)\n",
        "              clf.fit(X=x[0],y=y[0])\n",
        "              lst_res.append(clf.predict(x[1])[0])\n",
        "              lst_coef.append(sum(clf.coef_**2))\n",
        "\n",
        "          ## write to dictionary\n",
        "          dic_res[c].append(lst_res)\n",
        "          dic_coef[c].append(lst_coef)\n",
        "        # update\n",
        "        counter += 1\n",
        "\n",
        "    # write new or concatenate dataframes (results)\n",
        "    for c in combs:\n",
        "        dic_res[c] = pd.DataFrame(dic_res[c], columns=ret_std.index[c[0]:idx_end+1], index=range(seed_start,counter_stop)).T\n",
        "        dic_coef[c] = pd.DataFrame(dic_coef[c], columns=ret_std.index[c[0]:idx_end+1], index=range(seed_start,counter_stop)).T\n",
        "\n",
        "    return dic_res,dic_coef"
      ],
      "metadata": {
        "id": "k3zTCc50R9Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Plot Figure 10"
      ],
      "metadata": {
        "id": "XLpa2RRjSC3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fig10(results = None, T = None, P = None, alpha = None, roll_window = None, begin = None, end = None):\n",
        "    ## Determine start and end of recession periods\n",
        "    from itertools import groupby\n",
        "    from operator import itemgetter\n",
        "\n",
        "    # pull recession data from FRED as dummy variables for a given month\n",
        "    nber = web.DataReader([\"USREC\"], \"fred\", start = begin, end = end)\n",
        "    _sth = np.where(nber.USREC == 1)[0]\n",
        "    rec = []\n",
        "    for k,g in groupby(enumerate(_sth), lambda ix: ix[0] - ix[1]):\n",
        "        rec.append(list(map(itemgetter(1),g)))\n",
        "    rec = [(nber.index[i[0]], nber.index[i[-1]]) for i in rec]\n",
        "    rec_b = [i[0] for i in rec]\n",
        "    rec_e = [i[1] for i in rec]\n",
        "\n",
        "    #fig, ax = plt.subplots()\n",
        "    # relevant combinations\n",
        "    plt.figure(dpi=2000)\n",
        "    lbl = []\n",
        "    cmbs = list(itertools.product(T,[P],[alpha]))\n",
        "    for n,c in enumerate(cmbs):\n",
        "        use = results[c].rolling(roll_window).mean().mean(axis=1).dropna()\n",
        "        use = use.loc[begin:end]\n",
        "        # plot\n",
        "        plt.plot(use, lw = 0.65, alpha = 0.8)\n",
        "        lbl.append(\"$\\hat{\\pi},T =$\" + str(int(c[0])))\n",
        "        rec_patches = [plt.axvspan(x1, x2, alpha=0.4, color='grey', zorder=2) for x1, x2 in zip(rec_b, rec_e)]\n",
        "        #rec_handle = Line2D([0], [0], color='orange', alpha=0.4, lw=4, label='NBER Recession')\n",
        "\n",
        "    #plt.legend(handles= [rec_handle])\n",
        "    lbl.append(\"NBER Recession\")\n",
        "    plt.legend(lbl)\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "yUdG6frfSH7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}